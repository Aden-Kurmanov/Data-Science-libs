{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b881af",
   "metadata": {},
   "source": [
    "Задание 1\n",
    "Импортируйте библиотеки pandas и numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f5540997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1439e4",
   "metadata": {},
   "source": [
    "Загрузите \"Boston House Prices dataset\" из встроенных наборов данных библиотеки sklearn. Создайте датафреймы X и y из этих данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "09a45afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "feature_names = boston.feature_names\n",
    "\n",
    "x = pd.DataFrame(data, columns=feature_names)\n",
    "y = pd.DataFrame(target, columns=['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda459c2",
   "metadata": {},
   "source": [
    "Разбейте эти датафреймы на тренировочные (X_train, y_train) и тестовые (X_test, y_test) с помощью функции train_test_split так, чтобы размер тестовой выборки составлял 30% от всех данных, при этом аргумент random_state должен быть равен 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e33ff366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143304da",
   "metadata": {},
   "source": [
    "Создайте модель линейной регрессии под названием lr с помощью класса LinearRegression из модуля sklearn.linear_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20dc94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7a80e",
   "metadata": {},
   "source": [
    "Обучите модель на тренировочных данных (используйте все признаки) и сделайте предсказание на тестовых."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fafbc904",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fdb8cb",
   "metadata": {},
   "source": [
    "Вычислите R2 полученных предказаний с помощью r2_score из модуля sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d1397264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.711226005748496"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_1 = r2_score(y_test, y_pred)\n",
    "r2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f6c4a",
   "metadata": {},
   "source": [
    "Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240fcf8",
   "metadata": {},
   "source": [
    "Создайте модель под названием model с помощью RandomForestRegressor из модуля sklearn.ensemble.\n",
    "Сделайте агрумент n_estimators равным 1000, max_depth должен быть равен 12 и random_state сделайте равным 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c7e75ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=12, n_estimators=1000, random_state=42)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=1000, max_depth=12, random_state=42)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3828d930",
   "metadata": {},
   "source": [
    "Обучите модель на тренировочных данных аналогично тому, как вы обучали модель LinearRegression,\n",
    "но при этом в метод fit вместо датафрейма y_train поставьте y_train.values[:, 0],\n",
    "чтобы получить из датафрейма одномерный массив Numpy,\n",
    "так как для класса RandomForestRegressor в данном методе для аргумента y предпочтительно применение массивов вместо датафрейма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d1500cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=12, n_estimators=1000, random_state=42)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train.values[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d998487",
   "metadata": {},
   "source": [
    "Сделайте предсказание на тестовых данных и посчитайте R2. Сравните с результатом из предыдущего задания.\n",
    "Напишите в комментариях к коду, какая модель в данном случае работает лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "954d5db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor лучше\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = model.predict(x_test)\n",
    "\n",
    "r2_2 = r2_score(y_test, y_pred2)\n",
    "if (r2_1 > r2_2):\n",
    "    print('LinearRegression лучше')\n",
    "else:\n",
    "    print('RandomForestRegressor лучше')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9e11f",
   "metadata": {},
   "source": [
    "*Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed6d845",
   "metadata": {},
   "source": [
    "Вызовите документацию для класса RandomForestRegressor,\n",
    "найдите информацию об атрибуте feature_importances_.\n",
    "С помощью этого атрибута найдите сумму всех показателей важности,\n",
    "установите, какие два признака показывают наибольшую важность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "81a99154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestRegressor in module sklearn.ensemble._forest:\n",
      "\n",
      "class RandomForestRegressor(ForestRegressor)\n",
      " |  RandomForestRegressor(n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest regressor.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of classifying\n",
      " |  decision trees on various sub-samples of the dataset and uses averaging\n",
      " |  to improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"mse\", \"mae\"}, default=\"mse\"\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"mse\" for the mean squared error, which is equal to variance\n",
      " |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      " |      absolute error.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `round(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, default=None\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 1.0 (renaming of 0.25).\n",
      " |         Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      whether to use out-of-bag samples to estimate\n",
      " |      the R^2 on unseen data.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0, 1)`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : DecisionTreeRegressor\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeRegressor\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_prediction_ : ndarray of shape (n_samples,)\n",
      " |      Prediction computed with out-of-bag estimate on the training set.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor, ExtraTreesRegressor\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  The default value ``max_features=\"auto\"`` uses ``n_features``\n",
      " |  rather than ``n_features / 3``. The latter was originally suggested in\n",
      " |  [1], whereas the former was more recently justified empirically in [2].\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      " |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      " |  ...                        random_state=0, shuffle=False)\n",
      " |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      " |  >>> regr.fit(X, y)\n",
      " |  RandomForestRegressor(...)\n",
      " |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      " |  [-8.32987858]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestRegressor\n",
      " |      ForestRegressor\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestRegressor:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict regression target for X.\n",
      " |      \n",
      " |      The predicted regression target of an input sample is computed as the\n",
      " |      mean predicted regression targets of the trees in the forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination :math:`R^2` of the\n",
      " |      prediction.\n",
      " |      \n",
      " |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      " |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      " |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      " |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      " |      can be negative (because the model can be arbitrarily worse). A\n",
      " |      constant model that always predicts the expected value of `y`,\n",
      " |      disregarding the input features, would get a :math:`R^2` score of\n",
      " |      0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "df996bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_imp = model.feature_importances_.sum()\n",
    "sum_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "76adea43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEYCAYAAABSnD3BAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAda0lEQVR4nO3df7wddX3n8de7wTykiFblApoQiZAtRQWl10AXW0spFKw24C9AF6oVY3bB37hm3cfSVdYVbLVURWOkLOpKo63GphIJ/qBSi2CCIBAKbh4RTYyWgKhYrRB47x/fuWVycnLv3Nw5Jznj+/l43EfOfGe+85lzc+7nfOc73/mObBMREd31K7v7ACIiYrCS6CMiOi6JPiKi45LoIyI6Lok+IqLjkugjIjpur919AP3st99+Pvjgg3f3YUREjIwbb7zxHttj/dbtkYn+4IMPZt26dbv7MCIiRoak7+xsXbpuIiI6Lok+IqLjkugjIjouiT4iouOS6CMiOi6JPiKi45LoIyI6Lok+IqLj9sgbpiIifhkcvPTKade568I/nHadtOgjIjouiT4iouOS6CMiOq5Ropd0kqQ7JW2QtHSS7Z4t6SFJL55u3YiIGIwpE72kWcAlwMnA4cAZkg7fyXYXAWumWzciIganSYt+IbDB9kbbDwArgEV9tnst8Gng7l2oGxERA9JkeOUcYFNteTNwdH0DSXOAU4HfA549nboR0zGs4WgRXdKkRa8+Ze5Zvhh4q+2HdqFu2VBaLGmdpHVbt25tcFgREdFEkxb9ZuCg2vJcYEvPNuPACkkA+wHPk7StYV0AbC8HlgOMj4/3/TKIiIjpa5Lo1wILJM0HvgecDrysvoHt+ROvJV0OfM72ZyXtNVXdiIgYrCkTve1tks6ljKaZBVxme72kJdX6ZdOt286hR0REE43murG9GljdU9Y3wdt+xVR1IyJieHJnbERExyXRR0R0XBJ9RETHJdFHRHRcEn1ERMcl0UdEdFwSfURExyXRR0R0XBJ9RETHJdFHRHRcEn1ERMcl0UdEdFwSfURExyXRR0R0XBJ9RETHJdFHRHRco0Qv6SRJd0raIGlpn/WLJN0i6ebqAd/Pqa27S9KtE+vaPPiIiJjalE+YkjQLuAQ4gfKw77WSVtm+vbbZl4BVti3pCOBTwGG19cfZvqfF446IiIaatOgXAhtsb7T9ALACWFTfwPZPbbta3AcwERGxR2iS6OcAm2rLm6uy7Ug6VdIdwJXAn9RWGbha0o2SFs/kYCMiYvqaJHr1KduhxW57pe3DgFOAC2qrjrV9FHAycI6k3+kbRFpc9e+v27p1a4PDioiIJpok+s3AQbXlucCWnW1s+1rgEEn7Vctbqn/vBlZSuoL61Vtue9z2+NjYWMPDj4iIqTRJ9GuBBZLmS5oNnA6sqm8g6VBJql4fBcwG7pW0j6R9q/J9gBOB29p8AxERMbkpR93Y3ibpXGANMAu4zPZ6SUuq9cuAFwFnSXoQ+DlwWjUC5wBgZfUdsBdwhe2rBvReIiKijykTPYDt1cDqnrJltdcXARf1qbcROHKGxxgRETOQO2MjIjouiT4iouOS6CMiOi6JPiKi45LoIyI6Lok+IqLjkugjIjouiT4iouOS6CMiOi6JPiKi45LoIyI6Lok+IqLjkugjIjouiT4iouOS6CMiOi6JPiKi4xolekknSbpT0gZJS/usXyTpFkk3Vw/4fk7TuhERMVhTJnpJs4BLgJOBw4EzJB3es9mXgCNtPxP4E+DSadSNiIgBatKiXwhssL3R9gPACmBRfQPbP7XtanEfwE3rRkTEYDVJ9HOATbXlzVXZdiSdKukO4EpKq75x3YiIGJwmiV59yrxDgb3S9mHAKcAF06kLIGlx1b+/buvWrQ0OKyIimmiS6DcDB9WW5wJbdrax7WuBQyTtN526tpfbHrc9PjY21uCwIiKiiSaJfi2wQNJ8SbOB04FV9Q0kHSpJ1eujgNnAvU3qRkTEYO011Qa2t0k6F1gDzAIus71e0pJq/TLgRcBZkh4Efg6cVl2c7Vt3QO8lIiL6mDLRA9heDazuKVtWe30RcFHTuhERMTy5MzYiouOS6CMiOi6JPiKi45LoIyI6Lok+IqLjkugjIjouiT4iouOS6CMiOi6JPiKi45LoIyI6Lok+IqLjkugjIjouiT4iouOS6CMiOi6JPiKi45LoIyI6rlGil3SSpDslbZC0tM/6l0u6pfq5TtKRtXV3SbpV0s2S1rV58BERMbUpnzAlaRZwCXAC5WHfayWtsn17bbNvA8+1fZ+kk4HlwNG19cfZvqfF446IiIaatOgXAhtsb7T9ALACWFTfwPZ1tu+rFq8H5rZ7mBERsauaJPo5wKba8uaqbGdeBXy+tmzgakk3Slo8/UOMiIiZaPJwcPUpc98NpeMoif45teJjbW+RtD/wBUl32L62T93FwGKAefPmNTisiIhookmLfjNwUG15LrCldyNJRwCXAots3ztRbntL9e/dwEpKV9AObC+3PW57fGxsrPk7iIiISTVJ9GuBBZLmS5oNnA6sqm8gaR7wGeBM29+qle8jad+J18CJwG1tHXxERExtyq4b29sknQusAWYBl9leL2lJtX4ZcD7wROCDkgC22R4HDgBWVmV7AVfYvmog7yQiIvpq0keP7dXA6p6yZbXXZwNn96m3ETiytzwiIoYnd8ZGRHRcEn1ERMcl0UdEdFwSfURExyXRR0R0XBJ9RETHJdFHRHRcEn1ERMcl0UdEdFwSfURExyXRR0R0XBJ9RETHJdFHRHRcEn1ERMcl0UdEdFwSfURExzVK9JJOknSnpA2SlvZZ/3JJt1Q/10k6smndiIgYrCkTvaRZwCXAycDhwBmSDu/Z7NvAc20fAVwALJ9G3YiIGKAmLfqFwAbbG20/AKwAFtU3sH2d7fuqxeuBuU3rRkTEYDVJ9HOATbXlzVXZzrwK+Pwu1o2IiJY1eTi4+pS574bScZRE/5xdqLsYWAwwb968BocVERFNNGnRbwYOqi3PBbb0biTpCOBSYJHte6dTF8D2ctvjtsfHxsaaHHtERDTQJNGvBRZImi9pNnA6sKq+gaR5wGeAM21/azp1IyJisKbsurG9TdK5wBpgFnCZ7fWSllTrlwHnA08EPigJYFvVOu9bd0DvJSIi+mjSR4/t1cDqnrJltddnA2c3rRsREcOTO2MjIjouiT4iouOS6CMiOi6JPiKi45LoIyI6Lok+IqLjkugjIjouiT4iouOS6CMiOi6JPiKi45LoIyI6Lok+IqLjkugjIjouiT4iouOS6CMiOi6JPiKi4xolekknSbpT0gZJS/usP0zS1yT9QtJ5PevuknSrpJslrWvrwCMiopkpnzAlaRZwCXAC5WHfayWtsn17bbMfAq8DTtnJbo6zfc8MjzUiInZBkxb9QmCD7Y22HwBWAIvqG9i+2/Za4MEBHGNERMxAk0Q/B9hUW95clTVl4GpJN0paPJ2Di4iImWvycHD1KfM0Yhxre4uk/YEvSLrD9rU7BClfAosB5s2bN43dR0TEZJq06DcDB9WW5wJbmgawvaX6925gJaUrqN92y22P2x4fGxtruvuIiJhCk0S/Flggab6k2cDpwKomO5e0j6R9J14DJwK37erBRkTE9E3ZdWN7m6RzgTXALOAy2+slLanWL5N0ILAOeCzwsKQ3AIcD+wErJU3EusL2VQN5JxER0VeTPnpsrwZW95Qtq73+AaVLp9dPgCNncoARETEzuTM2IqLjkugjIjouiT4iouOS6CMiOi6JPiKi45LoIyI6Lok+IqLjkugjIjouiT4iouOS6CMiOi6JPiKi45LoIyI6Lok+IqLjkugjIjouiT4iouOS6CMiOq5Ropd0kqQ7JW2QtLTP+sMkfU3SLySdN526ERExWFMmekmzgEuAkymPBzxD0uE9m/0QeB3w57tQNyIiBqhJi34hsMH2RtsPACuARfUNbN9tey3w4HTrRkTEYDVJ9HOATbXlzVVZEzOpGxERLWiS6NWnzA3337iupMWS1klat3Xr1oa7j4iIqTRJ9JuBg2rLc4EtDfffuK7t5bbHbY+PjY013H1EREylSaJfCyyQNF/SbOB0YFXD/c+kbkREtGCvqTawvU3SucAaYBZwme31kpZU65dJOhBYBzwWeFjSG4DDbf+kX90BvZeIiOhjykQPYHs1sLqnbFnt9Q8o3TKN6kZExPDkztiIiI5Loo+I6Lgk+oiIjkuij4jouCT6iIiOS6KPiOi4JPqIiI5Loo+I6Lgk+oiIjkuij4jouCT6iIiOS6KPiOi4JPqIiI5rNHtlRLTv4KVXTrvOXRf+4QCOJLouLfqIiI5Loo+I6LhGiV7SSZLulLRB0tI+6yXpfdX6WyQdVVt3l6RbJd0saV2bBx8REVObso9e0izgEuAEysO+10paZfv22mYnAwuqn6OBD1X/TjjO9j2tHXVERDTWpEW/ENhge6PtB4AVwKKebRYBH3NxPfBrkp7U8rFGRMQuaJLo5wCbasubq7Km2xi4WtKNkhbv6oFGRMSuaTK8Un3KPI1tjrW9RdL+wBck3WH72h2ClC+BxQDz5s1rcFgREdFEkxb9ZuCg2vJcYEvTbWxP/Hs3sJLSFbQD28ttj9seHxsba3b0ERExpSaJfi2wQNJ8SbOB04FVPdusAs6qRt8cA/zY9vcl7SNpXwBJ+wAnAre1ePwRETGFKbtubG+TdC6wBpgFXGZ7vaQl1fplwGrgecAG4GfAK6vqBwArJU3EusL2Va2/i4iI2KlGUyDYXk1J5vWyZbXXBs7pU28jcOQMjzEiImYgd8ZGRHRcEn1ERMcl0UdEdFwSfUREx43UfPSZvzsiYvrSoo+I6Lgk+oiIjkuij4jouCT6iIiOS6KPiOi4JPqIiI4bqeGVERHTHWadIdZJ9NGSrt3jkGQSXZJEHxHRo2sNlyT6iGhF15Jjl+RibERExzVq0Us6CfhLyhOmLrV9Yc96VeufR3nC1Ctsf6NJ3V9m6QeOiGGYMtFLmgVcApxAeQj4WkmrbN9e2+xkYEH1czTwIeDohnUjYoDSpRJNum4WAhtsb7T9ALACWNSzzSLgYy6uB35N0pMa1o2IiAFq0nUzB9hUW95MabVPtc2chnX3OF1qAXXpvUTErlF5rvckG0gvAf7A9tnV8pnAQtuvrW1zJfAu21+tlr8E/FfgqVPVre1jMbC4Wvx14M5pvI/9gHumsf2uSpw9M0bi7LkxEmd4MZ5ie6zfiiYt+s3AQbXlucCWhtvMblAXANvLgeUNjmcHktbZHt+Vuokz2Dhdei9di9Ol99K1OG3HaNJHvxZYIGm+pNnA6cCqnm1WAWepOAb4se3vN6wbEREDNGWL3vY2SecCayhDJC+zvV7Skmr9MmA1ZWjlBsrwyldOVncg7yQiIvpqNI7e9mpKMq+XLau9NnBO07oDsEtdPokzlDhdei9di9Ol99K1OK3GmPJibEREjLZMgRAR0XFJ9BERHZdEv4eR9ChJz5K0/+4+ljZJGqmZUiU9dpJ184Z5LKNC0rMnWXfmMI9llAzj8zRyffSSXjjZetufaSnOWVPE+VhLcZYB769GMj0O+BrwEPAE4Dzbf91CjFcD/2D7/1UT0F0GvAi4i9oEdC3E+XvgXNvf6Sn/feBi209vKc77Jltv+3UtxPiG7aOq11+yfXy/dYMg6YnA7wDftX1ji/vdizIv1WFV0T8DV9ne1tL+bwH+Cfhvtn9UlT0d+CDwQ9untBGnFm9/yiCQpwEGbgc+aPtf2ozTJ+5+wL1uKXkO+vMEozkf/d8CN1c/AKqtM9BKogf6tU4EvIAytUMriR74bdtLqtevBL5l+xRJBwKfB2ac6IHXA5dXr88AjgDmA8+izCz62y3EgDKX0TWS/gp4NzAGXAzMA/64pRgAS4DbgE9RbsDT5Jvvkvo+nzDJupkHkj4HLLV9WzVH1DeAdcAhkpbbvriFGE8GrgG+D9xEeQ/PB94j6TjbfW9knKajgLcAN0m6AHgGZdj1m21/roX9/ztJxwJXUD7XH6O8n6OAGyS93PY/tRTnGOBC4IfABcDHKXet/oqks2xf1UaYFvYxOdsj9QOcSkko64D/ARw6hJgC/hNwK/BJ4IgW931T7fWVlBb2DutmGOPm2usrgNfXlr/R8u/qccCHKfdUfIcyrYVajvFESrK/BvgCcDbw+JZjfKPf6wH9ztbXXr+NMkEgwL7ALS3FuBx4Q5/y1wEfbfn9vAV4mHLH/JPb3HctxvXAs/qUPxO4ocU464ATgZcA9wHHVOWHtfj3eTfwvp39tBFj5Fr0tlcCKyXtQ5kJ8z3Vqe5/t/2VNmNVp7qvAN4M3AC82PZ05uBp4keSng98DzgWeFUt9t4txXi4aineBxwPvLO2rq0YEw6nzFr6dWAcOIBy5vhgWwFs3wssA5ZJmkM5S1kv6a22P95SmP0lvYnyJT/xmmq573wiM1D/3RwPfATA9v2SHm4pxjG2X9FbaPt9klr5TEs6hNJN8xDwG5RuomslvdP2/2kjRs1jbd/UW2j7Zkn7thhnL9tXA0h6h8vsvNi+o/SCtuLnQGtddP2MXKKv+Tfgx8BPKF0Dj25z55LOoXR5fAk4yT39zi16DeWb+0BKi+sHVfnxlBZ+G86ntExmAatc3Z0s6bnAxpZiIOlSyunzf7H9terL+O3ANyW9YeIPpsV4R1GS/AmUbq42/1g+QmlR974GuLTFOACbJL2W0gI+CrgKQNLewKNaivHzSdb9rKUYayhdUH9bLd8p6VPAeyWdbfvYluJAed7R423f11P4BNodZFL/ou39HbZ1gfNe2x9taV99jeLF2OMof9wLgS8CK2yvG0CchymnVFvZ/j9UlJuBj2g75iBVZwj71v8wJP0qMMv2/S3FeCPlVPOhnvJnUC6StXItQNLbKf3L/0zpxmvtguLuUF1UfAfwJOCSWgvyOOA3bf95CzE2Auf1WwW82/YhLcR4jO2f7mTd79v+4kxj1Pa3GHg15T1NDCb4TeAiylQrH24pzkPAv1J+T3vzyJeigEfbnvEXsaTrbR/Tp/xY4GW2+846MK0YI5joHwZuAb5KScDbvQG3MOKiirOE0rLq9ws6zfa7W4rz/p4YpkxPeo2raZ/bVo28OQ54GfAC2we0uO+Bj4SoPgMbeaSFNfH7a+1LWNLTgENsr6qW/4Jy/QHgA25ppNKwSJq068T2KwcU9xBKw+x0tzTqqrbv51OmQ39aVbQe+DPbf99mnGGS9EzK3+VLgW8Dn7H9/hnvdwQT/SuY5JSprVOg6pv8K8CZtr/Xs6614VCS+o1GeQLlP/qTbmHERS3W0ZQP0alVjHMoXTn3TVqx+f7rIyFu5JGREH8MtDkS4imTrW+jm60aKvou29dVy7dTLv7/KvAitzhUsIo12Wf6j9qKtZP4B7T8Rfwk4DTKZ+0I4F2UhHVrWzG6RNJ/oMzsewZwL2XAx3m2J/2cTyvGqCX6YZF0E+XC0vnAm2z/TX2d7WcNOP7ewHVtxJH0TsoXx3cpwzVXAutsz5/pvnviXA/8596LZFUr5cO2B/p0MZVnFJ9u+xMt7Gu7+cDrp9eSvmr7OTONUdv3cydb3/Yggyrm4yj3UrwM+A3bc1rY56spyWouZejrp4C/a/tzVsXqPRPeTltn9sNQnaH+I/Aq2xuqso22n9pWjJG7GDvE1o9tf0TSV4BPSHoecI7tn00Wvy22f97iVf3FlCd2fQj4nO1/kzSI9zCUkRAqd62eQ7mfYRVliOW5lP7am4EZJ3q2v/hKTx9qq3ct1xO5pLGqbGubMap97w38ESW5H0V5j6cA17YU4hLKDX8vm7huNqDPGZTBBV3xIkqL/hpJV1GuO7U6tn7kEj0w4wtT02H7W5J+C/hflBtBJr1jtg3VhdMzKaMw2nAgZSzwGcDFkq4B9pa0V8sXMYc1EuLjlKGiX6OMoX8L5Wlmi2zf3FKMLZKOtn1DvbC6gaaNm4u2I+lPgddS/sB/RdI2yh3T72hp/5+g3G17NfAB4MvABtv/0Mb+K3MpSeu9kg6gtOjbGjXU69dtv21A+x6qniHjpwBvBA6Q9CFgZSuj1WY6EH9P+gGObXFfN/Up+13KRcD7W4xzP2WI6P21n3+h/JG0frMJZRjqi4FPV3GuaHHfiylPFXsupbW4b/U7uwF4TYtxbq29nkVJ+vu2/HtaSLkY9qeUu6FfAPzPqmxhy7HeSDkrmV8reypluOIbW4rxTcoghvOAg6qyjS2/j/pNZnOrWDdSRkf970HFGvUf4PI+ZU+gDL3+chsxRq6PvuqHfSnltP0ql9vGn0+5o3Bvt9R3LukU25/tU/54StK6sI04u1PVnfJCtziGdxgjIXovhrd5cbwnzgFsP4JoPSUhn+EWhrzV4twEnGD7np7yMeDqFj/Th1G6bU6jDB0+DHiGH7l3Y6b7v6nfsVYXG8+w/fY24lT7/CalEdG3i8P2D9uKNWiD+vxuF2MEE/3llAeOfx04mnKb/W9RbtT47O47sl2nHSebuh1Y4/Ymm3rTZOttv7eNOMNSG9sM249vnhheudOZJ3cx3rMo3V4TQ94+bfsDLe7/Nu9k6OFk62YYc5yS9F8MbLb9H1vY52Zgp5+lNj9nkn5BuZu8X6K3W7yQOWiS7qB8vnb2pTXjobyj2Ec/Tplr5mFJj6aMOT+0rVbJsGnnk029V+1NNlW/sPgaylw0E1r7ppd0/iSrbfuCNuLYntXGfiazkyFvsn3cAMI9sIvrdpnLxdJ1kpZSvsDaMAt4DDtJvi3FmHB7W2c6e4A5wHvY+e/t92YaYBRb9EM5bR+W6gzlZveMl5f0OspdkW3O+jjQoaGS3tyneB/K/D1PtP2YQcQdhGEMeavFqp+hbLeK9u6+7DdS6RxKP/o3bS9qIcbQ/hYn+xy3fV/AoA1juPYotugPU5n3GsofwiG1ZTxiUxMwhMmmenc9gH2WHdvvmXhd9f+/njL18gpKi2WUDHzI24RhnKGw85FKp7i9kUqDn273EX+5XeCe+wIoX2hRGcVEfyRlRsRNPeVPYQDD3oZgGJNNDU01lPJNwMuBjwJHuaU7b4fJwxjyNlxPtf0MYGLyuXuAeW5pnqPK8VNv0g7blw/hvoBheWt9QdKjgKcD37N9dxsBRjHR/wXwNu/4FKOxat0LdstR7brHqf9TswS0clFR0q080pI/tH4GBO2dBUn6M+CFwHLKaI6+E1yNEtv/SrkB6xPVl9hLgKWU8eij5N+nQrb9kKRvt5zkhzrSZUj3BQzLCyV9z32eMiepnafMjWAf/WQjFG6daLWMCg1hsilJC5jkLGiiD7qFOA8DvwC20X/Gz1ZHw0Rzwx6pNGjV8EpRni71SdubBnUNZdAkrbf9tOr1G4Dfde0pc230349ii36yeefbfojGwLWRyBsYylmQ7Txsfg81pOsAQ2P7yNp9AV+UdDewr6QDR3AEXn1k1QnA3wDY/kFb06CMYqJfK+nVtj9SL5T0Kgb8lJZBmGJKBbudJyYdbPuW3kLb6yQd3ML+I4bO9h2USQfPr90X8HVJrdwXMEQ/0oCfMjeKXTcHUGZffIBHEvs4ZQTBqaP2bV7NwrdDMdVDyG3P+MtY0gbbh053XcSokTQbeKnt/7u7j6Wp6p6NiafMXWz78qr8D4ATbfcbtjy9GKOW6CeoPH1noq9+ve0v787jaYPKedrLKVfhbwfe2a8lvgv7/WvKnBn9zoJOtH3aTGNEDNMw7gvYE6g8gvPiGe9nVBN9l2jHh5C/yy0+hLxrZ0ERkv6OR+4LOB54POXz/PoW7wvY7SR91/a8Ge8niX730vYPIb+w94Jpy7E6dxYUv5zqI+yqiQ4HcV/Abidpk+2DZryfJPrdSx17CHnEMHRtKpSdSYu+IzSE559GdE2X7guQdD/9pyYRZer1mQ/ISKKPiFEj6VG2H5x6y4DRHEffKVN8m49UyyRiiG6gzG8TDSTR72a2W3todsQvkWHOlDnykugjYhSNTfbktDafZtUFSfQRMYome5pV9MjF2IgYOV0dTjkomW0wIkZRWvLTkBZ9RIwcSU+mPNT8UOBW4K9sb9u9R7XnSqKPiJEj6ZOUp2b9I3Ay8B3br9+9R7XnSqKPiJHTM9fNXsDX02e/c+mjj4hRVH8GbrpsppAWfUSMnC7NdTMMSfQRER2XrpuIiI5Loo+I6Lgk+oiIjkuij4jouCT6iIiO+/9M5n7PnEkxVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "imp = pd.Series(model.feature_importances_, index=feature_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "imp.plot.bar(ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
